"""Здесь я сопоставляю три нормы векторов (Евклид, Манхэттен(городская) и норму Бесконечности(Чебышёва))
и оцениваю полученные результаты, разбирая тем самым применимость каждой из метрик """

import math as m
#формируем изначальные оценки пользователей для книг(пример)
user_1 = [5,3,4,4]
user_2 = [3,1,2,3]
user_3 = [4,3,4,3]

#высчитываем разницу между 1 и 2 пользователем, а также между 1 и 3 пользователем. 
diff_u1_u2 = [x - y for x, y in zip(user_1, user_2)]
diff_u1_u3 = [x - y for x, y in zip(user_1, user_3)]

def l1_norm(v):
    """Функция высчитает Манхэттенскою норму"""
    return sum(abs(x) for x in v)

def l2_norm(v):
    """Функция высчитает Евклидову норму"""
    return round(m.sqrt(sum(x**2 for x in v)), 3)

def linf_norm(v):
    """Функция высчитает норму бесконечности"""
    return max(abs(x) for x in v)

print(f'Разница между user_1 и user_2: {diff_u1_u2}')
print(f'Разница между user_1 и user_3: {diff_u1_u3} \n')

print(f'Городская норма для user_1 и user_2: {l1_norm(diff_u1_u2)}')
print(f'Евклидова норма для user_1 и user_2: {l2_norm(diff_u1_u2)}')
print(f'Норма бесконечности вектора для user_1 и user_2: {linf_norm(diff_u1_u2)} \n')

print(f'Городская норма для user_1 и user_2: {l1_norm(diff_u1_u3)}')
print(f'Евклидова норма для user_1 и user_2: {l2_norm(diff_u1_u3)}')
print(f'Норма бесконечности вектора для user_1 и user_2: {linf_norm(diff_u1_u3)}')

#Отказ от numpy обоснован необходимостью тщательного разбора подсчетов и бОльшим соприкосновением с программой

"""Выводы, сделанные в процессе работы:

    1. Хорошая выборка данных - залог успеха. 
    Даже такой игрушечный пример при мельчайшем изменении начинает выдавать совсем другие результаты. К формированию данных следует относиться с должным вниманием.

    2. Манхэттенская норма показалась мне самым тщательным, поскольку он действительно иллюстрирует "суммарную" разницу. Мне кажется, 
    именно его следует использовать в системах рекомендаций. Объект имеет вектор [3,4,15,8]. Пользователь вывел себя на отметку [4, 2,11, 9]. 
    Манхэттеном пробегаемся по разнице [1, 2, 4, 1]. Суммируем и получаем 8. В целом, не такая большая разница. Можно советовать. 
    Вот в этом, мне кажется, заключен действительный потенциал. Особенно если рекомендуем что-то тонкое, многогранное (книги, кино).

    3. Норма бесконечности тоже показалась интересным инструментом. В этом примере с [3,4,15,8] и [4, 2,11, 9]. Мы получим результат 4. 
    Что также предполагает допустимость к рекомендации. Правда, я думаю, что в таких тонких вещах, как литература, лучше всё-таки опираться на Манхэттен.
    Если представить, что каждая координата - это позиция на оси категорий (комедийность, драматичность и тд.) такие "округления" могут сыграть злую шутку. 
    Норма бесконечности - хороший вариант, если пользователь уже движется достаточно однородно: он предпочитает приключенческую литературу 
    и находится на отметке в [1,1,12,1]. Тогда для книги с координатами [2, 2, 9, 2] не потребуются лишние действия (суммирование координат). 
    Мы увидим 3 и смело предложим книгу.

    4. Евклид при первом рассмотрении показался неочевидным. Но его можно включать точечно, для намеренной рекомендации чего-то непохожего на то, что обычно оценивает 
    пользователь. Юзер любит антиутопии? А что, если предложить "Каштанку"? Возможно, он оценит, сместится с намеченного курса и получит уникальный опыт.
    exploration–exploitation trade-off в чистом виде. 

    Как мне кажется, следует выбрать основную норму и включать остальные ситуативно, для формирования необычных рекомендаций и действительно интересного взаимодействия
    с системой рекомендаций. 
    
"""